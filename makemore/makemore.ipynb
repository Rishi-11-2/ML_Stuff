{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4M21JTKogwI",
        "outputId": "cf70ef44-c5bd-4bd9-92a1-7d0809eaf1dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v5MnrmD02Gta"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P-b56Ji71j-1"
      },
      "outputs": [],
      "source": [
        "word=open('names.txt','r').read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dYzGrVjm1lgJ"
      },
      "outputs": [],
      "source": [
        "b={}\n",
        "for w in word:\n",
        "    chs=['<S>']+list(w)+['<E>']\n",
        "    for ch1,ch2 in zip(chs,chs[1:]):\n",
        "        bigram=(ch1,ch2)\n",
        "        b[bigram]=b.get(bigram,0)+1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je_RZRrV1mxm",
        "outputId": "2d133525-9695-44c3-db3b-4a6ce542a734"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('n', '<E>'), 6763),\n",
              " (('a', '<E>'), 6640),\n",
              " (('a', 'n'), 5438),\n",
              " (('<S>', 'a'), 4410),\n",
              " (('e', '<E>'), 3983),\n",
              " (('a', 'r'), 3264),\n",
              " (('e', 'l'), 3248),\n",
              " (('r', 'i'), 3033),\n",
              " (('n', 'a'), 2977),\n",
              " (('<S>', 'k'), 2963),\n",
              " (('l', 'e'), 2921),\n",
              " (('e', 'n'), 2675),\n",
              " (('l', 'a'), 2623),\n",
              " (('m', 'a'), 2590),\n",
              " (('<S>', 'm'), 2538),\n",
              " (('a', 'l'), 2528),\n",
              " (('i', '<E>'), 2489),\n",
              " (('l', 'i'), 2480),\n",
              " (('i', 'a'), 2445),\n",
              " (('<S>', 'j'), 2422),\n",
              " (('o', 'n'), 2411),\n",
              " (('h', '<E>'), 2409),\n",
              " (('r', 'a'), 2356),\n",
              " (('a', 'h'), 2332),\n",
              " (('h', 'a'), 2244),\n",
              " (('y', 'a'), 2143),\n",
              " (('i', 'n'), 2126),\n",
              " (('<S>', 's'), 2055),\n",
              " (('a', 'y'), 2050),\n",
              " (('y', '<E>'), 2007),\n",
              " (('e', 'r'), 1958),\n",
              " (('n', 'n'), 1906),\n",
              " (('y', 'n'), 1826),\n",
              " (('k', 'a'), 1731),\n",
              " (('n', 'i'), 1725),\n",
              " (('r', 'e'), 1697),\n",
              " (('<S>', 'd'), 1690),\n",
              " (('i', 'e'), 1653),\n",
              " (('a', 'i'), 1650),\n",
              " (('<S>', 'r'), 1639),\n",
              " (('a', 'm'), 1634),\n",
              " (('l', 'y'), 1588),\n",
              " (('<S>', 'l'), 1572),\n",
              " (('<S>', 'c'), 1542),\n",
              " (('<S>', 'e'), 1531),\n",
              " (('j', 'a'), 1473),\n",
              " (('r', '<E>'), 1377),\n",
              " (('n', 'e'), 1359),\n",
              " (('l', 'l'), 1345),\n",
              " (('i', 'l'), 1345),\n",
              " (('i', 's'), 1316),\n",
              " (('l', '<E>'), 1314),\n",
              " (('<S>', 't'), 1308),\n",
              " (('<S>', 'b'), 1306),\n",
              " (('d', 'a'), 1303),\n",
              " (('s', 'h'), 1285),\n",
              " (('d', 'e'), 1283),\n",
              " (('e', 'e'), 1271),\n",
              " (('m', 'i'), 1256),\n",
              " (('s', 'a'), 1201),\n",
              " (('s', '<E>'), 1169),\n",
              " (('<S>', 'n'), 1146),\n",
              " (('a', 's'), 1118),\n",
              " (('y', 'l'), 1104),\n",
              " (('e', 'y'), 1070),\n",
              " (('o', 'r'), 1059),\n",
              " (('a', 'd'), 1042),\n",
              " (('t', 'a'), 1027),\n",
              " (('<S>', 'z'), 929),\n",
              " (('v', 'i'), 911),\n",
              " (('k', 'e'), 895),\n",
              " (('s', 'e'), 884),\n",
              " (('<S>', 'h'), 874),\n",
              " (('r', 'o'), 869),\n",
              " (('e', 's'), 861),\n",
              " (('z', 'a'), 860),\n",
              " (('o', '<E>'), 855),\n",
              " (('i', 'r'), 849),\n",
              " (('b', 'r'), 842),\n",
              " (('a', 'v'), 834),\n",
              " (('m', 'e'), 818),\n",
              " (('e', 'i'), 818),\n",
              " (('c', 'a'), 815),\n",
              " (('i', 'y'), 779),\n",
              " (('r', 'y'), 773),\n",
              " (('e', 'm'), 769),\n",
              " (('s', 't'), 765),\n",
              " (('h', 'i'), 729),\n",
              " (('t', 'e'), 716),\n",
              " (('n', 'd'), 704),\n",
              " (('l', 'o'), 692),\n",
              " (('a', 'e'), 692),\n",
              " (('a', 't'), 687),\n",
              " (('s', 'i'), 684),\n",
              " (('e', 'a'), 679),\n",
              " (('d', 'i'), 674),\n",
              " (('h', 'e'), 674),\n",
              " (('<S>', 'g'), 669),\n",
              " (('t', 'o'), 667),\n",
              " (('c', 'h'), 664),\n",
              " (('b', 'e'), 655),\n",
              " (('t', 'h'), 647),\n",
              " (('v', 'a'), 642),\n",
              " (('o', 'l'), 619),\n",
              " (('<S>', 'i'), 591),\n",
              " (('i', 'o'), 588),\n",
              " (('e', 't'), 580),\n",
              " (('v', 'e'), 568),\n",
              " (('a', 'k'), 568),\n",
              " (('a', 'a'), 556),\n",
              " (('c', 'e'), 551),\n",
              " (('a', 'b'), 541),\n",
              " (('i', 't'), 541),\n",
              " (('<S>', 'y'), 535),\n",
              " (('t', 'i'), 532),\n",
              " (('s', 'o'), 531),\n",
              " (('m', '<E>'), 516),\n",
              " (('d', '<E>'), 516),\n",
              " (('<S>', 'p'), 515),\n",
              " (('i', 'c'), 509),\n",
              " (('k', 'i'), 509),\n",
              " (('o', 's'), 504),\n",
              " (('n', 'o'), 496),\n",
              " (('t', '<E>'), 483),\n",
              " (('j', 'o'), 479),\n",
              " (('u', 's'), 474),\n",
              " (('a', 'c'), 470),\n",
              " (('n', 'y'), 465),\n",
              " (('e', 'v'), 463),\n",
              " (('s', 's'), 461),\n",
              " (('m', 'o'), 452),\n",
              " (('i', 'k'), 445),\n",
              " (('n', 't'), 443),\n",
              " (('i', 'd'), 440),\n",
              " (('j', 'e'), 440),\n",
              " (('a', 'z'), 435),\n",
              " (('i', 'g'), 428),\n",
              " (('i', 'm'), 427),\n",
              " (('r', 'r'), 425),\n",
              " (('d', 'r'), 424),\n",
              " (('<S>', 'f'), 417),\n",
              " (('u', 'r'), 414),\n",
              " (('r', 'l'), 413),\n",
              " (('y', 's'), 401),\n",
              " (('<S>', 'o'), 394),\n",
              " (('e', 'd'), 384),\n",
              " (('a', 'u'), 381),\n",
              " (('c', 'o'), 380),\n",
              " (('k', 'y'), 379),\n",
              " (('d', 'o'), 378),\n",
              " (('<S>', 'v'), 376),\n",
              " (('t', 't'), 374),\n",
              " (('z', 'e'), 373),\n",
              " (('z', 'i'), 364),\n",
              " (('k', '<E>'), 363),\n",
              " (('g', 'h'), 360),\n",
              " (('t', 'r'), 352),\n",
              " (('k', 'o'), 344),\n",
              " (('t', 'y'), 341),\n",
              " (('g', 'e'), 334),\n",
              " (('g', 'a'), 330),\n",
              " (('l', 'u'), 324),\n",
              " (('b', 'a'), 321),\n",
              " (('d', 'y'), 317),\n",
              " (('c', 'k'), 316),\n",
              " (('<S>', 'w'), 307),\n",
              " (('k', 'h'), 307),\n",
              " (('u', 'l'), 301),\n",
              " (('y', 'e'), 301),\n",
              " (('y', 'r'), 291),\n",
              " (('m', 'y'), 287),\n",
              " (('h', 'o'), 287),\n",
              " (('w', 'a'), 280),\n",
              " (('s', 'l'), 279),\n",
              " (('n', 's'), 278),\n",
              " (('i', 'z'), 277),\n",
              " (('u', 'n'), 275),\n",
              " (('o', 'u'), 275),\n",
              " (('n', 'g'), 273),\n",
              " (('y', 'd'), 272),\n",
              " (('c', 'i'), 271),\n",
              " (('y', 'o'), 271),\n",
              " (('i', 'v'), 269),\n",
              " (('e', 'o'), 269),\n",
              " (('o', 'm'), 261),\n",
              " (('r', 'u'), 252),\n",
              " (('f', 'a'), 242),\n",
              " (('b', 'i'), 217),\n",
              " (('s', 'y'), 215),\n",
              " (('n', 'c'), 213),\n",
              " (('h', 'y'), 213),\n",
              " (('p', 'a'), 209),\n",
              " (('r', 't'), 208),\n",
              " (('q', 'u'), 206),\n",
              " (('p', 'h'), 204),\n",
              " (('h', 'r'), 204),\n",
              " (('j', 'u'), 202),\n",
              " (('g', 'r'), 201),\n",
              " (('p', 'e'), 197),\n",
              " (('n', 'l'), 195),\n",
              " (('y', 'i'), 192),\n",
              " (('g', 'i'), 190),\n",
              " (('o', 'd'), 190),\n",
              " (('r', 's'), 190),\n",
              " (('r', 'd'), 187),\n",
              " (('h', 'l'), 185),\n",
              " (('s', 'u'), 185),\n",
              " (('a', 'x'), 182),\n",
              " (('e', 'z'), 181),\n",
              " (('e', 'k'), 178),\n",
              " (('o', 'v'), 176),\n",
              " (('a', 'j'), 175),\n",
              " (('o', 'h'), 171),\n",
              " (('u', 'e'), 169),\n",
              " (('m', 'm'), 168),\n",
              " (('a', 'g'), 168),\n",
              " (('h', 'u'), 166),\n",
              " (('x', '<E>'), 164),\n",
              " (('u', 'a'), 163),\n",
              " (('r', 'm'), 162),\n",
              " (('a', 'w'), 161),\n",
              " (('f', 'i'), 160),\n",
              " (('z', '<E>'), 160),\n",
              " (('u', '<E>'), 155),\n",
              " (('u', 'm'), 154),\n",
              " (('e', 'c'), 153),\n",
              " (('v', 'o'), 153),\n",
              " (('e', 'h'), 152),\n",
              " (('p', 'r'), 151),\n",
              " (('d', 'd'), 149),\n",
              " (('o', 'a'), 149),\n",
              " (('w', 'e'), 149),\n",
              " (('w', 'i'), 148),\n",
              " (('y', 'm'), 148),\n",
              " (('z', 'y'), 147),\n",
              " (('n', 'z'), 145),\n",
              " (('y', 'u'), 141),\n",
              " (('r', 'n'), 140),\n",
              " (('o', 'b'), 140),\n",
              " (('k', 'l'), 139),\n",
              " (('m', 'u'), 139),\n",
              " (('l', 'd'), 138),\n",
              " (('h', 'n'), 138),\n",
              " (('u', 'd'), 136),\n",
              " (('<S>', 'x'), 134),\n",
              " (('t', 'l'), 134),\n",
              " (('a', 'f'), 134),\n",
              " (('o', 'e'), 132),\n",
              " (('e', 'x'), 132),\n",
              " (('e', 'g'), 125),\n",
              " (('f', 'e'), 123),\n",
              " (('z', 'l'), 123),\n",
              " (('u', 'i'), 121),\n",
              " (('v', 'y'), 121),\n",
              " (('e', 'b'), 121),\n",
              " (('r', 'h'), 121),\n",
              " (('j', 'i'), 119),\n",
              " (('o', 't'), 118),\n",
              " (('d', 'h'), 118),\n",
              " (('h', 'm'), 117),\n",
              " (('c', 'l'), 116),\n",
              " (('o', 'o'), 115),\n",
              " (('y', 'c'), 115),\n",
              " (('o', 'w'), 114),\n",
              " (('o', 'c'), 114),\n",
              " (('f', 'r'), 114),\n",
              " (('b', '<E>'), 114),\n",
              " (('m', 'b'), 112),\n",
              " (('z', 'o'), 110),\n",
              " (('i', 'b'), 110),\n",
              " (('i', 'u'), 109),\n",
              " (('k', 'r'), 109),\n",
              " (('g', '<E>'), 108),\n",
              " (('y', 'v'), 106),\n",
              " (('t', 'z'), 105),\n",
              " (('b', 'o'), 105),\n",
              " (('c', 'y'), 104),\n",
              " (('y', 't'), 104),\n",
              " (('u', 'b'), 103),\n",
              " (('u', 'c'), 103),\n",
              " (('x', 'a'), 103),\n",
              " (('b', 'l'), 103),\n",
              " (('o', 'y'), 103),\n",
              " (('x', 'i'), 102),\n",
              " (('i', 'f'), 101),\n",
              " (('r', 'c'), 99),\n",
              " (('c', '<E>'), 97),\n",
              " (('m', 'r'), 97),\n",
              " (('n', 'u'), 96),\n",
              " (('o', 'p'), 95),\n",
              " (('i', 'h'), 95),\n",
              " (('k', 's'), 95),\n",
              " (('l', 's'), 94),\n",
              " (('u', 'k'), 93),\n",
              " (('<S>', 'q'), 92),\n",
              " (('d', 'u'), 92),\n",
              " (('s', 'm'), 90),\n",
              " (('r', 'k'), 90),\n",
              " (('i', 'x'), 89),\n",
              " (('v', '<E>'), 88),\n",
              " (('y', 'k'), 86),\n",
              " (('u', 'w'), 86),\n",
              " (('g', 'u'), 85),\n",
              " (('b', 'y'), 83),\n",
              " (('e', 'p'), 83),\n",
              " (('g', 'o'), 83),\n",
              " (('s', 'k'), 82),\n",
              " (('u', 't'), 82),\n",
              " (('a', 'p'), 82),\n",
              " (('e', 'f'), 82),\n",
              " (('i', 'i'), 82),\n",
              " (('r', 'v'), 80),\n",
              " (('f', '<E>'), 80),\n",
              " (('t', 'u'), 78),\n",
              " (('y', 'z'), 78),\n",
              " (('<S>', 'u'), 78),\n",
              " (('l', 't'), 77),\n",
              " (('r', 'g'), 76),\n",
              " (('c', 'r'), 76),\n",
              " (('i', 'j'), 76),\n",
              " (('w', 'y'), 73),\n",
              " (('z', 'u'), 73),\n",
              " (('l', 'v'), 72),\n",
              " (('h', 't'), 71),\n",
              " (('j', '<E>'), 71),\n",
              " (('x', 't'), 70),\n",
              " (('o', 'i'), 69),\n",
              " (('e', 'u'), 69),\n",
              " (('o', 'k'), 68),\n",
              " (('b', 'd'), 65),\n",
              " (('a', 'o'), 63),\n",
              " (('p', 'i'), 61),\n",
              " (('s', 'c'), 60),\n",
              " (('d', 'l'), 60),\n",
              " (('l', 'm'), 60),\n",
              " (('a', 'q'), 60),\n",
              " (('f', 'o'), 60),\n",
              " (('p', 'o'), 59),\n",
              " (('n', 'k'), 58),\n",
              " (('w', 'n'), 58),\n",
              " (('u', 'h'), 58),\n",
              " (('e', 'j'), 55),\n",
              " (('n', 'v'), 55),\n",
              " (('s', 'r'), 55),\n",
              " (('o', 'z'), 54),\n",
              " (('i', 'p'), 53),\n",
              " (('l', 'b'), 52),\n",
              " (('i', 'q'), 52),\n",
              " (('w', '<E>'), 51),\n",
              " (('m', 'c'), 51),\n",
              " (('s', 'p'), 51),\n",
              " (('e', 'w'), 50),\n",
              " (('k', 'u'), 50),\n",
              " (('v', 'r'), 48),\n",
              " (('u', 'g'), 47),\n",
              " (('o', 'x'), 45),\n",
              " (('u', 'z'), 45),\n",
              " (('z', 'z'), 45),\n",
              " (('j', 'h'), 45),\n",
              " (('b', 'u'), 45),\n",
              " (('o', 'g'), 44),\n",
              " (('n', 'r'), 44),\n",
              " (('f', 'f'), 44),\n",
              " (('n', 'j'), 44),\n",
              " (('z', 'h'), 43),\n",
              " (('c', 'c'), 42),\n",
              " (('r', 'b'), 41),\n",
              " (('x', 'o'), 41),\n",
              " (('b', 'h'), 41),\n",
              " (('p', 'p'), 39),\n",
              " (('x', 'l'), 39),\n",
              " (('h', 'v'), 39),\n",
              " (('b', 'b'), 38),\n",
              " (('m', 'p'), 38),\n",
              " (('x', 'x'), 38),\n",
              " (('u', 'v'), 37),\n",
              " (('x', 'e'), 36),\n",
              " (('w', 'o'), 36),\n",
              " (('c', 't'), 35),\n",
              " (('z', 'm'), 35),\n",
              " (('t', 's'), 35),\n",
              " (('m', 's'), 35),\n",
              " (('c', 'u'), 35),\n",
              " (('o', 'f'), 34),\n",
              " (('u', 'x'), 34),\n",
              " (('k', 'w'), 34),\n",
              " (('p', '<E>'), 33),\n",
              " (('g', 'l'), 32),\n",
              " (('z', 'r'), 32),\n",
              " (('d', 'n'), 31),\n",
              " (('g', 't'), 31),\n",
              " (('g', 'y'), 31),\n",
              " (('h', 's'), 31),\n",
              " (('x', 's'), 31),\n",
              " (('g', 's'), 30),\n",
              " (('x', 'y'), 30),\n",
              " (('y', 'g'), 30),\n",
              " (('d', 'm'), 30),\n",
              " (('d', 's'), 29),\n",
              " (('h', 'k'), 29),\n",
              " (('y', 'x'), 28),\n",
              " (('q', '<E>'), 28),\n",
              " (('g', 'n'), 27),\n",
              " (('y', 'b'), 27),\n",
              " (('g', 'w'), 26),\n",
              " (('n', 'h'), 26),\n",
              " (('k', 'n'), 26),\n",
              " (('g', 'g'), 25),\n",
              " (('d', 'g'), 25),\n",
              " (('l', 'c'), 25),\n",
              " (('r', 'j'), 25),\n",
              " (('w', 'u'), 25),\n",
              " (('l', 'k'), 24),\n",
              " (('m', 'd'), 24),\n",
              " (('s', 'w'), 24),\n",
              " (('s', 'n'), 24),\n",
              " (('h', 'd'), 24),\n",
              " (('w', 'h'), 23),\n",
              " (('y', 'j'), 23),\n",
              " (('y', 'y'), 23),\n",
              " (('r', 'z'), 23),\n",
              " (('d', 'w'), 23),\n",
              " (('w', 'r'), 22),\n",
              " (('t', 'n'), 22),\n",
              " (('l', 'f'), 22),\n",
              " (('y', 'h'), 22),\n",
              " (('r', 'w'), 21),\n",
              " (('s', 'b'), 21),\n",
              " (('m', 'n'), 20),\n",
              " (('f', 'l'), 20),\n",
              " (('w', 's'), 20),\n",
              " (('k', 'k'), 20),\n",
              " (('h', 'z'), 20),\n",
              " (('g', 'd'), 19),\n",
              " (('l', 'h'), 19),\n",
              " (('n', 'm'), 19),\n",
              " (('x', 'z'), 19),\n",
              " (('u', 'f'), 19),\n",
              " (('f', 't'), 18),\n",
              " (('l', 'r'), 18),\n",
              " (('p', 't'), 17),\n",
              " (('t', 'c'), 17),\n",
              " (('k', 't'), 17),\n",
              " (('d', 'v'), 17),\n",
              " (('u', 'p'), 16),\n",
              " (('p', 'l'), 16),\n",
              " (('l', 'w'), 16),\n",
              " (('p', 's'), 16),\n",
              " (('o', 'j'), 16),\n",
              " (('r', 'q'), 16),\n",
              " (('y', 'p'), 15),\n",
              " (('l', 'p'), 15),\n",
              " (('t', 'v'), 15),\n",
              " (('r', 'p'), 14),\n",
              " (('l', 'n'), 14),\n",
              " (('e', 'q'), 14),\n",
              " (('f', 'y'), 14),\n",
              " (('s', 'v'), 14),\n",
              " (('u', 'j'), 14),\n",
              " (('v', 'l'), 14),\n",
              " (('q', 'a'), 13),\n",
              " (('u', 'y'), 13),\n",
              " (('q', 'i'), 13),\n",
              " (('w', 'l'), 13),\n",
              " (('p', 'y'), 12),\n",
              " (('y', 'f'), 12),\n",
              " (('c', 'q'), 11),\n",
              " (('j', 'r'), 11),\n",
              " (('n', 'w'), 11),\n",
              " (('n', 'f'), 11),\n",
              " (('t', 'w'), 11),\n",
              " (('m', 'z'), 11),\n",
              " (('u', 'o'), 10),\n",
              " (('f', 'u'), 10),\n",
              " (('l', 'z'), 10),\n",
              " (('h', 'w'), 10),\n",
              " (('u', 'q'), 10),\n",
              " (('j', 'y'), 10),\n",
              " (('s', 'z'), 10),\n",
              " (('s', 'd'), 9),\n",
              " (('j', 'l'), 9),\n",
              " (('d', 'j'), 9),\n",
              " (('k', 'm'), 9),\n",
              " (('r', 'f'), 9),\n",
              " (('h', 'j'), 9),\n",
              " (('v', 'n'), 8),\n",
              " (('n', 'b'), 8),\n",
              " (('i', 'w'), 8),\n",
              " (('h', 'b'), 8),\n",
              " (('b', 's'), 8),\n",
              " (('w', 't'), 8),\n",
              " (('w', 'd'), 8),\n",
              " (('v', 'v'), 7),\n",
              " (('v', 'u'), 7),\n",
              " (('j', 's'), 7),\n",
              " (('m', 'j'), 7),\n",
              " (('f', 's'), 6),\n",
              " (('l', 'g'), 6),\n",
              " (('l', 'j'), 6),\n",
              " (('j', 'w'), 6),\n",
              " (('n', 'x'), 6),\n",
              " (('y', 'q'), 6),\n",
              " (('w', 'k'), 6),\n",
              " (('g', 'm'), 6),\n",
              " (('x', 'u'), 5),\n",
              " (('m', 'h'), 5),\n",
              " (('m', 'l'), 5),\n",
              " (('j', 'm'), 5),\n",
              " (('c', 's'), 5),\n",
              " (('j', 'v'), 5),\n",
              " (('n', 'p'), 5),\n",
              " (('d', 'f'), 5),\n",
              " (('x', 'd'), 5),\n",
              " (('z', 'b'), 4),\n",
              " (('f', 'n'), 4),\n",
              " (('x', 'c'), 4),\n",
              " (('m', 't'), 4),\n",
              " (('t', 'm'), 4),\n",
              " (('z', 'n'), 4),\n",
              " (('z', 't'), 4),\n",
              " (('p', 'u'), 4),\n",
              " (('c', 'z'), 4),\n",
              " (('b', 'n'), 4),\n",
              " (('z', 's'), 4),\n",
              " (('f', 'w'), 4),\n",
              " (('d', 't'), 4),\n",
              " (('j', 'd'), 4),\n",
              " (('j', 'c'), 4),\n",
              " (('y', 'w'), 4),\n",
              " (('v', 'k'), 3),\n",
              " (('x', 'w'), 3),\n",
              " (('t', 'j'), 3),\n",
              " (('c', 'j'), 3),\n",
              " (('q', 'w'), 3),\n",
              " (('g', 'b'), 3),\n",
              " (('o', 'q'), 3),\n",
              " (('r', 'x'), 3),\n",
              " (('d', 'c'), 3),\n",
              " (('g', 'j'), 3),\n",
              " (('x', 'f'), 3),\n",
              " (('z', 'w'), 3),\n",
              " (('d', 'k'), 3),\n",
              " (('u', 'u'), 3),\n",
              " (('m', 'v'), 3),\n",
              " (('c', 'x'), 3),\n",
              " (('l', 'q'), 3),\n",
              " (('p', 'b'), 2),\n",
              " (('t', 'g'), 2),\n",
              " (('q', 's'), 2),\n",
              " (('t', 'x'), 2),\n",
              " (('f', 'k'), 2),\n",
              " (('b', 't'), 2),\n",
              " (('j', 'n'), 2),\n",
              " (('k', 'c'), 2),\n",
              " (('z', 'k'), 2),\n",
              " (('s', 'j'), 2),\n",
              " (('s', 'f'), 2),\n",
              " (('z', 'j'), 2),\n",
              " (('n', 'q'), 2),\n",
              " (('f', 'z'), 2),\n",
              " (('h', 'g'), 2),\n",
              " (('w', 'w'), 2),\n",
              " (('k', 'j'), 2),\n",
              " (('j', 'k'), 2),\n",
              " (('w', 'm'), 2),\n",
              " (('z', 'c'), 2),\n",
              " (('z', 'v'), 2),\n",
              " (('w', 'f'), 2),\n",
              " (('q', 'm'), 2),\n",
              " (('k', 'z'), 2),\n",
              " (('j', 'j'), 2),\n",
              " (('z', 'p'), 2),\n",
              " (('j', 't'), 2),\n",
              " (('k', 'b'), 2),\n",
              " (('m', 'w'), 2),\n",
              " (('h', 'f'), 2),\n",
              " (('c', 'g'), 2),\n",
              " (('t', 'f'), 2),\n",
              " (('h', 'c'), 2),\n",
              " (('q', 'o'), 2),\n",
              " (('k', 'd'), 2),\n",
              " (('k', 'v'), 2),\n",
              " (('s', 'g'), 2),\n",
              " (('z', 'd'), 2),\n",
              " (('q', 'r'), 1),\n",
              " (('d', 'z'), 1),\n",
              " (('p', 'j'), 1),\n",
              " (('q', 'l'), 1),\n",
              " (('p', 'f'), 1),\n",
              " (('q', 'e'), 1),\n",
              " (('b', 'c'), 1),\n",
              " (('c', 'd'), 1),\n",
              " (('m', 'f'), 1),\n",
              " (('p', 'n'), 1),\n",
              " (('w', 'b'), 1),\n",
              " (('p', 'c'), 1),\n",
              " (('h', 'p'), 1),\n",
              " (('f', 'h'), 1),\n",
              " (('b', 'j'), 1),\n",
              " (('f', 'g'), 1),\n",
              " (('z', 'g'), 1),\n",
              " (('c', 'p'), 1),\n",
              " (('p', 'k'), 1),\n",
              " (('p', 'm'), 1),\n",
              " (('x', 'n'), 1),\n",
              " (('s', 'q'), 1),\n",
              " (('k', 'f'), 1),\n",
              " (('m', 'k'), 1),\n",
              " (('x', 'h'), 1),\n",
              " (('g', 'f'), 1),\n",
              " (('v', 'b'), 1),\n",
              " (('j', 'p'), 1),\n",
              " (('g', 'z'), 1),\n",
              " (('v', 'd'), 1),\n",
              " (('d', 'b'), 1),\n",
              " (('v', 'h'), 1),\n",
              " (('h', 'h'), 1),\n",
              " (('g', 'v'), 1),\n",
              " (('d', 'q'), 1),\n",
              " (('x', 'b'), 1),\n",
              " (('w', 'z'), 1),\n",
              " (('h', 'q'), 1),\n",
              " (('j', 'b'), 1),\n",
              " (('x', 'm'), 1),\n",
              " (('w', 'g'), 1),\n",
              " (('t', 'b'), 1),\n",
              " (('z', 'x'), 1)]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted(b.items(),key=lambda kv : -kv[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gRlLezxq1n97"
      },
      "outputs": [],
      "source": [
        "N=torch.zeros((27,27),dtype=torch.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ehz9BgKM1p8A"
      },
      "outputs": [],
      "source": [
        "chars=sorted(list(set(''.join(word))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p2q0a7cm1rJr"
      },
      "outputs": [],
      "source": [
        "stoi={s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.']=0\n",
        "itos={i:s for s,i in  stoi.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZDBjneLW1sOo"
      },
      "outputs": [],
      "source": [
        "b={}\n",
        "for w in word:\n",
        "    chs=['.']+list(w)+['.']\n",
        "    for ch1,ch2 in zip(chs,chs[1:]):\n",
        "      ix1=stoi[ch1]\n",
        "      ix2=stoi[ch2]\n",
        "      N[ix1,ix2]+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OJjWU4-T5_4k",
        "outputId": "86757833-e8bc-45b8-8fba-dadb0baa1ffd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
              "         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
              "          134,  535,  929],\n",
              "        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n",
              "         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n",
              "          182, 2050,  435],\n",
              "        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,\n",
              "          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,\n",
              "            0,   83,    0],\n",
              "        [  97,  815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,\n",
              "          116,    0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,\n",
              "            3,  104,    4],\n",
              "        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,\n",
              "           60,   30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,\n",
              "            0,  317,    1],\n",
              "        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,\n",
              "         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,\n",
              "          132, 1070,  181],\n",
              "        [  80,  242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,\n",
              "           20,    0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,\n",
              "            0,   14,    2],\n",
              "        [ 108,  330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,\n",
              "           32,    6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,\n",
              "            0,   31,    1],\n",
              "        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,\n",
              "          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,\n",
              "            0,  213,   20],\n",
              "        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,\n",
              "         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,\n",
              "           89,  779,  277],\n",
              "        [  71, 1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,\n",
              "            9,    5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,\n",
              "            0,   10,    0],\n",
              "        [ 363, 1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,\n",
              "          139,    9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,\n",
              "            0,  379,    2],\n",
              "        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,\n",
              "         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,\n",
              "            0, 1588,   10],\n",
              "        [ 516, 2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,\n",
              "            5,  168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,\n",
              "            0,  287,   11],\n",
              "        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,\n",
              "          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,\n",
              "            6,  465,  145],\n",
              "        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,\n",
              "          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,\n",
              "           45,  103,   54],\n",
              "        [  33,  209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,\n",
              "           16,    1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,\n",
              "            0,   12,    0],\n",
              "        [  28,   13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,\n",
              "            1,    2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,\n",
              "            0,    0,    0],\n",
              "        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,\n",
              "          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,\n",
              "            3,  773,   23],\n",
              "        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,\n",
              "          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,\n",
              "            0,  215,   10],\n",
              "        [ 483, 1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,\n",
              "          134,    4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,\n",
              "            2,  341,  105],\n",
              "        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,\n",
              "          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,\n",
              "           34,   13,   45],\n",
              "        [  88,  642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,\n",
              "           14,    0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,\n",
              "            0,  121,    0],\n",
              "        [  51,  280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,\n",
              "           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,\n",
              "            0,   73,    1],\n",
              "        [ 164,  103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,\n",
              "           39,    1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,\n",
              "           38,   30,   19],\n",
              "        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,\n",
              "         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,\n",
              "           28,   23,   78],\n",
              "        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,\n",
              "          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,\n",
              "            1,  147,   45]], dtype=torch.int32)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDbxmn0k31p9",
        "outputId": "f4a15e7e-82bc-4253-dcaa-e021795078aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n",
              "        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n",
              "        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p=N[0].float()\n",
        "p=p/p.sum()\n",
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT5s-pVy5NV-",
        "outputId": "b5ef3431-7a1b-4861-ffcf-ecfff3720fae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "g=torch.Generator().manual_seed(2147483647)\n",
        "ix=torch.multinomial(p,num_samples=1,replacement=True,generator=g).item()\n",
        "ix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dCaF72UXnimz"
      },
      "outputs": [],
      "source": [
        "P=(N+1).float()\n",
        "P/=P.sum(1,keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UCEUVNF2o-R5"
      },
      "outputs": [],
      "source": [
        "#WONT WORK\n",
        "#  27 27\n",
        "#  1  27   (as we are not keeping dims = True)\n",
        "\n",
        "# P=N.float()\n",
        "# P=P/P.sum(1)\n",
        "# P[:,0].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfRJ8tSbklK5",
        "outputId": "2649c7d7-7e0d-4318-d9c4-86318969f058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "braara.\n",
            "ke.\n"
          ]
        }
      ],
      "source": [
        "g=torch.Generator().manual_seed(2147483697)\n",
        "for _ in range(2):\n",
        "  ix=0\n",
        "  out = []\n",
        "  while True:\n",
        "    p=P[ix]\n",
        "    ix=torch.multinomial(p,num_samples=1,replacement=True,generator=g).item()\n",
        "    out.append(itos[ix])\n",
        "    if ix == 0 :\n",
        "      break\n",
        "  print(''.join(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp3wyK3EtGOC"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        " GOAL : maximize the likelihood of data w.r.t model parameters(statistical modelling)\n",
        " equivalent to maximizing the log likelihood because log is monotonic\n",
        " equivalent to minimizing the negative log likelihood\n",
        " equivalent to minimizing the average negative log likelihood\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvNyCL2alNux",
        "outputId": "6c63170c-b9de-46dc-8e59-1898b42aed4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-559951.5625)\n",
            "tensor(559951.5625)\n",
            "tensor(2.4544)\n"
          ]
        }
      ],
      "source": [
        "#likelihood is the product of all of these probabilities\n",
        "# log(a*b*c)= log(a)+log(b)+log(c)\n",
        "\n",
        "b={}\n",
        "log_likelihood=0.0\n",
        "n=0\n",
        "for w in word:\n",
        "    chs=['.']+list(w)+['.']\n",
        "    for ch1,ch2 in zip(chs,chs[1:]):\n",
        "      ix1=stoi[ch1]\n",
        "      ix2=stoi[ch2]\n",
        "      prob=P[ix1,ix2]\n",
        "      logprob=torch.log(prob)\n",
        "      log_likelihood+=logprob\n",
        "      n+=1\n",
        "      # print(f\"{ch1}{ch2} , {prob:.4f} , {logprob:.4f}\")\n",
        "print(log_likelihood)\n",
        "nil=-log_likelihood\n",
        "print(nil)\n",
        "print(nil/n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbOYM4BRu-UP"
      },
      "source": [
        "###TRIGRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gy41cIuvvKJR"
      },
      "outputs": [],
      "source": [
        "NN=torch.zeros((27,27,27),dtype=torch.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VdS9GBGtu_k7"
      },
      "outputs": [],
      "source": [
        "b={}\n",
        "for w in word:\n",
        "    chs=['.']+list(w)+['.']\n",
        "    for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
        "      ix1=stoi[ch1]\n",
        "      ix2=stoi[ch2]\n",
        "      ix3=stoi[ch3]\n",
        "      NN[ix1,ix2,ix3]+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FifbssKvdpL",
        "outputId": "cea3f0ac-4fd8-4194-8464-48a3811e66ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[  0,   0,   0,  ...,   0,   0,   0],\n",
              "         [  0, 207, 190,  ...,  27, 173, 152],\n",
              "         [  0, 169,   0,  ...,   0,   4,   0],\n",
              "         ...,\n",
              "         [  0,  57,   0,  ...,   1,  17,  11],\n",
              "         [  0, 246,   0,  ...,   0,   0,   2],\n",
              "         [  0, 456,   0,  ...,   0,  91,   1]],\n",
              "\n",
              "        [[  0,   0,   0,  ...,   0,   0,   0],\n",
              "         [ 40,   0,   5,  ...,   0,  20,  11],\n",
              "         [ 36,  28,  20,  ...,   0,  12,   0],\n",
              "         ...,\n",
              "         [ 11,   5,   0,  ...,  17,   6,   3],\n",
              "         [163, 389,  13,  ...,   0,  16,  40],\n",
              "         [ 38, 123,   0,  ...,   0,  12,  22]],\n",
              "\n",
              "        [[  0,   0,   0,  ...,   0,   0,   0],\n",
              "         [ 46,   5,   5,  ...,   4,  31,   4],\n",
              "         [  1,   8,   0,  ...,   0,   9,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0,  ...,   0,   0,   0],\n",
              "         [ 55,   4,   1,  ...,   0,   0,   0],\n",
              "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[  0,   0,   0,  ...,   0,   0,   0],\n",
              "         [ 10,   0,   2,  ...,   0,  10,   0],\n",
              "         [  0,   0,   0,  ...,   0,   1,   0],\n",
              "         ...,\n",
              "         [ 18,   3,   0,  ...,   0,   1,   0],\n",
              "         [  5,   4,   0,  ...,   0,   0,   0],\n",
              "         [  0,  16,   0,  ...,   0,   0,   0]],\n",
              "\n",
              "        [[  0,   0,   0,  ...,   0,   0,   0],\n",
              "         [716,  46,  10,  ...,   3,   6,  21],\n",
              "         [  2,   2,   0,  ...,   0,   0,   0],\n",
              "         ...,\n",
              "         [ 23,   1,   0,  ...,   1,   0,   0],\n",
              "         [  1,  18,   0,  ...,   0,   0,   0],\n",
              "         [  2,  27,   0,  ...,   1,   0,   0]],\n",
              "\n",
              "        [[  0,   0,   0,  ...,   0,   0,   0],\n",
              "         [ 98,  14,  40,  ...,   3,  97,   3],\n",
              "         [  0,   0,   0,  ...,   0,   0,   0],\n",
              "         ...,\n",
              "         [  1,   0,   0,  ...,   0,   0,   0],\n",
              "         [ 34,  27,   0,  ...,   0,   0,   1],\n",
              "         [  4,  13,   0,  ...,   0,   7,   0]]], dtype=torch.int32)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cxn4hBjNvUKG"
      },
      "outputs": [],
      "source": [
        "P=(NN+1).float()\n",
        "P/=P.sum(2,keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1fEYuZLwDas",
        "outputId": "f5ee2415-dc09-46f5-87af-ceef4e0614b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "braira.\n",
            "de.\n",
            "h.\n",
            "jiukie.\n",
            "h.\n",
            "n.\n",
            "mot.\n",
            "ry.\n",
            "ydissisamarlethiksvy.\n",
            "yafitaliona.\n"
          ]
        }
      ],
      "source": [
        "g=torch.Generator().manual_seed(2147483697)\n",
        "for _ in range(10):\n",
        "  ix1=0\n",
        "  ix2=1\n",
        "  out = []\n",
        "  while True:\n",
        "    p=P[ix1][ix2]\n",
        "    ix3=torch.multinomial(p,num_samples=1,replacement=True,generator=g).item()\n",
        "    out.append(itos[ix3])\n",
        "    if ix3 == 0 :\n",
        "      break\n",
        "    ix1=ix2\n",
        "    ix2=ix3\n",
        "  print(''.join(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU9qsWjavivC",
        "outputId": "8184da2b-893b-42a7-a283-f587ebcd531c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-410414.9688)\n",
            "tensor(410414.9688)\n",
            "tensor(2.0927)\n"
          ]
        }
      ],
      "source": [
        "#likelihood is the product of all of these probabilities\n",
        "# log(a*b*c)= log(a)+log(b)+log(c)\n",
        "\n",
        "b={}\n",
        "log_likelihood=0.0\n",
        "n=0\n",
        "for w in word:\n",
        "    chs=['.']+list(w)+['.']\n",
        "    for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
        "      ix1=stoi[ch1]\n",
        "      ix2=stoi[ch2]\n",
        "      ix3=stoi[ch3]\n",
        "      prob=P[ix1,ix2,ix3]\n",
        "      logprob=torch.log(prob)\n",
        "      log_likelihood+=logprob\n",
        "      n+=1\n",
        "      # print(f\"{ch1}{ch2} , {prob:.4f} , {logprob:.4f}\")\n",
        "print(log_likelihood)\n",
        "nil=-log_likelihood\n",
        "print(nil)\n",
        "print(nil/n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Zqi1jYr-wrIl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl9d7qO9oSDc"
      },
      "source": [
        "NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "x9eB_huJoTh4"
      },
      "outputs": [],
      "source": [
        "#create the trainig set of all bigrams(x,y)\n",
        "xs,ys=[] ,[]\n",
        "for w in word[:1]:\n",
        "    chs=['.']+list(w)+['.']\n",
        "    for ch1,ch2 in zip(chs,chs[1:]):\n",
        "      # print(ch1,ch2)\n",
        "      ix1=stoi[ch1]\n",
        "      ix2=stoi[ch2]\n",
        "      xs.append(ix1)\n",
        "      ys.append(ix2)\n",
        "\n",
        "xs=torch.tensor(xs)\n",
        "ys=torch.tensor(ys)\n",
        "\n",
        "# print(xs,ys)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cphSsDNqp_qA"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "xenc=F.one_hot(xs,num_classes=27).float()  # we want to feed  floating values to the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI_QrI8Jq99y",
        "outputId": "772d8bda-7fa5-47ea-b133-06850551dd95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 27])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xenc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJyoNikpqCqN",
        "outputId": "39984746-8cc5-43cf-dcaf-811dd4e1b172"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.0950,  0.6521,  0.7953,  0.8294,  0.3932,  1.2772,  1.0585, -0.8130,\n",
              "         -0.0170,  1.5280,  1.0834,  0.0530,  0.9910,  0.8806, -0.3309,  0.6359,\n",
              "          1.8753, -0.9402,  0.6251,  0.0173, -1.0657,  1.0862,  0.1646,  0.0140,\n",
              "         -1.2127,  0.1658,  0.8886],\n",
              "        [-0.8528, -1.3825, -0.5517,  1.3323, -0.4966, -0.0708, -0.3837,  0.9186,\n",
              "          0.6016,  0.8046,  1.8429,  0.9324,  1.8090, -0.4814,  2.9916, -0.5573,\n",
              "          0.2175, -0.9438,  0.5021,  0.8810,  0.3784, -2.5708, -2.5306, -0.4170,\n",
              "         -0.2810, -0.8524, -1.6967],\n",
              "        [-2.4856, -0.0637, -1.6918,  0.7430,  0.1133, -0.7453, -0.1503, -1.6353,\n",
              "         -0.1197, -1.0830,  1.1927, -1.7150, -0.8994,  1.0229, -0.0757, -0.3056,\n",
              "          0.6616,  0.9152, -1.5670, -0.7620,  0.1513,  1.3391, -0.8892, -1.3594,\n",
              "          0.0759,  0.4193,  1.4833],\n",
              "        [-2.4856, -0.0637, -1.6918,  0.7430,  0.1133, -0.7453, -0.1503, -1.6353,\n",
              "         -0.1197, -1.0830,  1.1927, -1.7150, -0.8994,  1.0229, -0.0757, -0.3056,\n",
              "          0.6616,  0.9152, -1.5670, -0.7620,  0.1513,  1.3391, -0.8892, -1.3594,\n",
              "          0.0759,  0.4193,  1.4833],\n",
              "        [ 1.0361, -0.2905, -2.3343, -0.7955, -0.8755, -2.2957, -0.0540, -1.2182,\n",
              "          0.0564,  1.4961, -0.3096,  1.1424, -1.1463, -1.5988, -0.9010,  0.5321,\n",
              "         -1.8721, -0.8370,  0.0826, -1.1912,  0.9154, -0.1313, -0.8289, -0.0282,\n",
              "         -1.0816, -0.4053,  0.3867]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W= torch.randn((27,27))\n",
        "xenc@W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4CUpOZedsb_f"
      },
      "outputs": [],
      "source": [
        "logits=xenc@W  # log counts\n",
        "\n",
        "counts=logits.exp()\n",
        "prob=counts/counts.sum(1,keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4mUx10usiU8",
        "outputId": "65bc2127-a625-45fc-fda4-1ffd5859d98c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prob[0].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN_-Ds7MuGhe",
        "outputId": "77f104c5-916d-4cd8-e856-61b17f828e78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0,  5, 13, 13,  1])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52ByOLkWuG_5",
        "outputId": "a9c6fbe3-7018-41ca-d0d8-e783d6c10878"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 5, 13, 13,  1,  0])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9a6B_lgOuHcm"
      },
      "outputs": [],
      "source": [
        "# randomly initialize 27 neurons weight ,each neuron receives 27  inputs\n",
        "g=torch.Generator().manual_seed(2147483647)\n",
        "W=torch.randn((27,27),generator=g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QaFSXaQ3uaM2"
      },
      "outputs": [],
      "source": [
        "xenc=F.one_hot(xs,num_classes=27).float() # input to the network : one-hot encoding\n",
        "logits=xenc@W  # predict log counts\n",
        "counts=logits.exp() # counts\n",
        "probs=counts/counts.sum(1,keepdim=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Hnq8SLDuxFG",
        "outputId": "40691fe0-0269-405d-f75a-b95fae6e7975"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 27])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KvvK_BgyuzaA",
        "outputId": "444181fa-fd7e-4f57-a8e4-bb20afdb8dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------\n",
            "bigram example 1: . e, indexes: 05\n",
            "input to the neural net : 0\n",
            "output probabilities from the neural net : tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
            "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
            "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
            "label(actual next character): 5\n",
            "probability assigned by the net to the correct character : 0.01228625513613224\n",
            "log likelihood : -4.399273872375488\n",
            "negative log likelihood 4.399273872375488\n",
            "=======================================================\n",
            "average negative log likelihood is : 0.8798547983169556\n",
            "-----------\n",
            "bigram example 2: e m, indexes: 513\n",
            "input to the neural net : 5\n",
            "output probabilities from the neural net : tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
            "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
            "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
            "label(actual next character): 13\n",
            "probability assigned by the net to the correct character : 0.018050700426101685\n",
            "log likelihood : -4.014570713043213\n",
            "negative log likelihood 4.014570713043213\n",
            "=======================================================\n",
            "average negative log likelihood is : 1.6827690601348877\n",
            "-----------\n",
            "bigram example 3: m m, indexes: 1313\n",
            "input to the neural net : 13\n",
            "output probabilities from the neural net : tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
            "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
            "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
            "label(actual next character): 13\n",
            "probability assigned by the net to the correct character : 0.026691533625125885\n",
            "log likelihood : -3.623408794403076\n",
            "negative log likelihood 3.623408794403076\n",
            "=======================================================\n",
            "average negative log likelihood is : 2.4074509143829346\n",
            "-----------\n",
            "bigram example 4: m a, indexes: 131\n",
            "input to the neural net : 13\n",
            "output probabilities from the neural net : tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
            "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
            "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
            "label(actual next character): 1\n",
            "probability assigned by the net to the correct character : 0.07367686182260513\n",
            "log likelihood : -2.6080665588378906\n",
            "negative log likelihood 2.6080665588378906\n",
            "=======================================================\n",
            "average negative log likelihood is : 2.9290642738342285\n",
            "-----------\n",
            "bigram example 5: a ., indexes: 10\n",
            "input to the neural net : 1\n",
            "output probabilities from the neural net : tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
            "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
            "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
            "label(actual next character): 0\n",
            "probability assigned by the net to the correct character : 0.014977526850998402\n",
            "log likelihood : -4.201204299926758\n",
            "negative log likelihood 4.201204299926758\n",
            "=======================================================\n",
            "average negative log likelihood is : 3.7693049907684326\n"
          ]
        }
      ],
      "source": [
        "nlls=torch.zeros(5)\n",
        "for  i in range(5):\n",
        "  x=xs[i].item() # input character index\n",
        "  y=ys[i].item() # label character index\n",
        "\n",
        "  print('-----------')\n",
        "  print(f'bigram example {i+1}: {itos[x]} {itos[y]}, indexes: {x}{y}')\n",
        "  print('input to the neural net :', x)\n",
        "  print('output probabilities from the neural net :', probs[i])\n",
        "  print('label(actual next character):',y)\n",
        "  p=probs[i,y]\n",
        "  print('probability assigned by the net to the correct character :',p.item())\n",
        "  logp=torch.log(p)\n",
        "  print('log likelihood :',logp.item())\n",
        "  nll=-logp\n",
        "  print('negative log likelihood',nll.item())\n",
        "  nlls[i]=nll\n",
        "\n",
        "\n",
        "  print(\"=======================================================\")\n",
        "  print('average negative log likelihood is :',nlls.mean().item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "P36RsLMfw9Cb"
      },
      "outputs": [],
      "source": [
        "# randomly initialize 27 neurons weight ,each neuron receives 27  inputs\n",
        "g=torch.Generator().manual_seed(2147483647)\n",
        "W=torch.randn((27,27),generator=g,requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Ev2PgbcHwA7a"
      },
      "outputs": [],
      "source": [
        "#forward pass\n",
        "xenc=F.one_hot(xs,num_classes=27).float() # input to the network : one-hot encoding\n",
        "logits=xenc@W  # predict log counts\n",
        "counts=logits.exp() # counts\n",
        "probs=counts/counts.sum(1,keepdim=True)\n",
        "loss=-probs[torch.arange(5),ys].log().mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6h2d5bNw3rB",
        "outputId": "67bd48df-df70-4b3f-b189-884c44a6fa43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(3.7693, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "yh5NOoIzxu4X"
      },
      "outputs": [],
      "source": [
        "#backward pass\n",
        "W.grad=None #set to zero\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL7s_0qVx6Rz",
        "outputId": "9cf4c241-89a2-453c-a046-77de78897066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([27, 27])\n"
          ]
        }
      ],
      "source": [
        "print(W.grad.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "3iHiePvmyTWN"
      },
      "outputs": [],
      "source": [
        "# update\n",
        "\n",
        "W.data+=-0.1*W.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M03bF0glyWpO",
        "outputId": "70a5388a-e5cc-410e-dc3f-50b99e58c79d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(3.7693, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvIt1vRHyrpL",
        "outputId": "cb4e7032-a14b-4e46-fc76-bfc4f73f180c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of examples: 228146\n"
          ]
        }
      ],
      "source": [
        "#create the trainig set of all bigrams(x,y)\n",
        "xs,ys=[] ,[]\n",
        "for w in word:\n",
        "    chs=['.']+list(w)+['.']\n",
        "    for ch1,ch2 in zip(chs,chs[1:]):\n",
        "      # print(ch1,ch2)\n",
        "      ix1=stoi[ch1]\n",
        "      ix2=stoi[ch2]\n",
        "      xs.append(ix1)\n",
        "      ys.append(ix2)\n",
        "\n",
        "xs=torch.tensor(xs)\n",
        "ys=torch.tensor(ys)\n",
        "num=xs.nelement()\n",
        "print('number of examples:',num)\n",
        "\n",
        "# print(xs,ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "yLNTeESAyytY"
      },
      "outputs": [],
      "source": [
        "# randomly initialize 27 neurons weight ,each neuron receives 27  inputs\n",
        "g=torch.Generator().manual_seed(2147483647)\n",
        "W=torch.randn((27,27),generator=g,requires_grad=True)\n",
        "\n",
        "xenc=F.one_hot(xs,num_classes=27).float() # input to the network : one-hot encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B2M9kTZZy6dr",
        "outputId": "c1e27e34-b2da-416f-a7d9-bc1b7c352049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss at the 0th iteration is: 3.7686190605163574\n",
            "\n",
            "Loss at the 10th iteration is: 3.6029648780822754\n",
            "\n",
            "Loss at the 20th iteration is: 3.4683403968811035\n",
            "\n",
            "Loss at the 30th iteration is: 3.357903242111206\n",
            "\n",
            "Loss at the 40th iteration is: 3.2670750617980957\n",
            "\n",
            "Loss at the 50th iteration is: 3.191946268081665\n",
            "\n",
            "Loss at the 60th iteration is: 3.1290879249572754\n",
            "\n",
            "Loss at the 70th iteration is: 3.075745105743408\n",
            "\n",
            "Loss at the 80th iteration is: 3.029876470565796\n",
            "\n",
            "Loss at the 90th iteration is: 2.9900121688842773\n",
            "\n",
            "Loss at the 100th iteration is: 2.955076217651367\n",
            "\n",
            "Loss at the 110th iteration is: 2.9242498874664307\n",
            "\n",
            "Loss at the 120th iteration is: 2.8968865871429443\n",
            "\n",
            "Loss at the 130th iteration is: 2.872462272644043\n",
            "\n",
            "Loss at the 140th iteration is: 2.8505427837371826\n",
            "\n",
            "Loss at the 150th iteration is: 2.83076810836792\n",
            "\n",
            "Loss at the 160th iteration is: 2.812838554382324\n",
            "\n",
            "Loss at the 170th iteration is: 2.7965023517608643\n",
            "\n",
            "Loss at the 180th iteration is: 2.7815499305725098\n",
            "\n",
            "Loss at the 190th iteration is: 2.7678046226501465\n",
            "\n",
            "Loss at the 200th iteration is: 2.7551190853118896\n",
            "\n",
            "Loss at the 210th iteration is: 2.7433691024780273\n",
            "\n",
            "Loss at the 220th iteration is: 2.7324490547180176\n",
            "\n",
            "Loss at the 230th iteration is: 2.7222704887390137\n",
            "\n",
            "Loss at the 240th iteration is: 2.712757110595703\n",
            "\n",
            "Loss at the 250th iteration is: 2.7038445472717285\n",
            "\n",
            "Loss at the 260th iteration is: 2.695476531982422\n",
            "\n",
            "Loss at the 270th iteration is: 2.687605142593384\n",
            "\n",
            "Loss at the 280th iteration is: 2.680187940597534\n",
            "\n",
            "Loss at the 290th iteration is: 2.6731884479522705\n",
            "\n",
            "Loss at the 300th iteration is: 2.6665737628936768\n",
            "\n",
            "Loss at the 310th iteration is: 2.6603150367736816\n",
            "\n",
            "Loss at the 320th iteration is: 2.654386520385742\n",
            "\n",
            "Loss at the 330th iteration is: 2.6487646102905273\n",
            "\n",
            "Loss at the 340th iteration is: 2.643428325653076\n",
            "\n",
            "Loss at the 350th iteration is: 2.6383588314056396\n",
            "\n",
            "Loss at the 360th iteration is: 2.633538246154785\n",
            "\n",
            "Loss at the 370th iteration is: 2.628950595855713\n",
            "\n",
            "Loss at the 380th iteration is: 2.6245808601379395\n",
            "\n",
            "Loss at the 390th iteration is: 2.620415687561035\n",
            "\n",
            "Loss at the 400th iteration is: 2.6164419651031494\n",
            "\n",
            "Loss at the 410th iteration is: 2.6126487255096436\n",
            "\n",
            "Loss at the 420th iteration is: 2.6090242862701416\n",
            "\n",
            "Loss at the 430th iteration is: 2.6055591106414795\n",
            "\n",
            "Loss at the 440th iteration is: 2.602243661880493\n",
            "\n",
            "Loss at the 450th iteration is: 2.5990684032440186\n",
            "\n",
            "Loss at the 460th iteration is: 2.59602689743042\n",
            "\n",
            "Loss at the 470th iteration is: 2.593109607696533\n",
            "\n",
            "Loss at the 480th iteration is: 2.59031081199646\n",
            "\n",
            "Loss at the 490th iteration is: 2.587623119354248\n",
            "\n",
            "Loss at the 500th iteration is: 2.58504056930542\n",
            "\n",
            "Loss at the 510th iteration is: 2.5825576782226562\n",
            "\n",
            "Loss at the 520th iteration is: 2.5801687240600586\n",
            "\n",
            "Loss at the 530th iteration is: 2.5778684616088867\n",
            "\n",
            "Loss at the 540th iteration is: 2.575653314590454\n",
            "\n",
            "Loss at the 550th iteration is: 2.5735175609588623\n",
            "\n",
            "Loss at the 560th iteration is: 2.571457624435425\n",
            "\n",
            "Loss at the 570th iteration is: 2.569469690322876\n",
            "\n",
            "Loss at the 580th iteration is: 2.5675504207611084\n",
            "\n",
            "Loss at the 590th iteration is: 2.5656957626342773\n",
            "\n",
            "Loss at the 600th iteration is: 2.5639030933380127\n",
            "\n",
            "Loss at the 610th iteration is: 2.5621695518493652\n",
            "\n",
            "Loss at the 620th iteration is: 2.5604915618896484\n",
            "\n",
            "Loss at the 630th iteration is: 2.558867931365967\n",
            "\n",
            "Loss at the 640th iteration is: 2.5572946071624756\n",
            "\n",
            "Loss at the 650th iteration is: 2.5557703971862793\n",
            "\n",
            "Loss at the 660th iteration is: 2.554292678833008\n",
            "\n",
            "Loss at the 670th iteration is: 2.5528595447540283\n",
            "\n",
            "Loss at the 680th iteration is: 2.551469326019287\n",
            "\n",
            "Loss at the 690th iteration is: 2.5501201152801514\n",
            "\n",
            "Loss at the 700th iteration is: 2.5488100051879883\n",
            "\n",
            "Loss at the 710th iteration is: 2.5475378036499023\n",
            "\n",
            "Loss at the 720th iteration is: 2.54630184173584\n",
            "\n",
            "Loss at the 730th iteration is: 2.545099973678589\n",
            "\n",
            "Loss at the 740th iteration is: 2.5439326763153076\n",
            "\n",
            "Loss at the 750th iteration is: 2.5427966117858887\n",
            "\n",
            "Loss at the 760th iteration is: 2.541691303253174\n",
            "\n",
            "Loss at the 770th iteration is: 2.540616273880005\n",
            "\n",
            "Loss at the 780th iteration is: 2.5395700931549072\n",
            "\n",
            "Loss at the 790th iteration is: 2.5385515689849854\n",
            "\n",
            "Loss at the 800th iteration is: 2.5375595092773438\n",
            "\n",
            "Loss at the 810th iteration is: 2.536593437194824\n",
            "\n",
            "Loss at the 820th iteration is: 2.5356523990631104\n",
            "\n",
            "Loss at the 830th iteration is: 2.5347352027893066\n",
            "\n",
            "Loss at the 840th iteration is: 2.5338408946990967\n",
            "\n",
            "Loss at the 850th iteration is: 2.5329692363739014\n",
            "\n",
            "Loss at the 860th iteration is: 2.5321195125579834\n",
            "\n",
            "Loss at the 870th iteration is: 2.5312905311584473\n",
            "\n",
            "Loss at the 880th iteration is: 2.530482053756714\n",
            "\n",
            "Loss at the 890th iteration is: 2.529693365097046\n",
            "\n",
            "Loss at the 900th iteration is: 2.528923511505127\n",
            "\n",
            "Loss at the 910th iteration is: 2.528172016143799\n",
            "\n",
            "Loss at the 920th iteration is: 2.5274384021759033\n",
            "\n",
            "Loss at the 930th iteration is: 2.526721954345703\n",
            "\n",
            "Loss at the 940th iteration is: 2.52602219581604\n",
            "\n",
            "Loss at the 950th iteration is: 2.525338649749756\n",
            "\n",
            "Loss at the 960th iteration is: 2.5246710777282715\n",
            "\n",
            "Loss at the 970th iteration is: 2.5240185260772705\n",
            "\n",
            "Loss at the 980th iteration is: 2.523380756378174\n",
            "\n",
            "Loss at the 990th iteration is: 2.5227577686309814\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(1000):\n",
        "  # forward pass\n",
        "  logits=xenc@W  # predict log counts\n",
        "  counts=logits.exp() # counts\n",
        "  probs=counts/counts.sum(1,keepdim=True)\n",
        "  loss=-probs[torch.arange(num),ys].log().mean() + 0.01 * (W**2).mean()\n",
        "\n",
        "  #backward pass\n",
        "  W.grad=None #set to zero\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "\n",
        "  W.data+=-2*W.grad\n",
        "  if i%10 == 0:\n",
        "    print(f\"Loss at the {i}th iteration is: {loss}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARIA0jsy2b2Y",
        "outputId": "09ca89c3-4c2a-4e49-dd87-c2f3c0bb44a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kelen.\n",
            "nxzanan.\n",
            "o.\n",
            "ayar.\n",
            "riman.\n"
          ]
        }
      ],
      "source": [
        "g=torch.Generator().manual_seed(214483647)\n",
        "for i in range(5):\n",
        "  out=[]\n",
        "  ix=0\n",
        "\n",
        "  while True:\n",
        "    # forward pass\n",
        "    xenc=F.one_hot(torch.tensor([ix]),num_classes=27).float()\n",
        "    logits=xenc@W  # predict log counts\n",
        "    counts=logits.exp() # counts\n",
        "    probs=counts/counts.sum(1,keepdim=True)\n",
        "\n",
        "    ix=torch.multinomial(probs,num_samples=1,replacement=True,generator=g).item()\n",
        "    out.append(itos[ix])\n",
        "\n",
        "    if ix==0:\n",
        "      break\n",
        "\n",
        "\n",
        "  print(''.join(out))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgsKDP7a0D9x"
      },
      "outputs": [],
      "source": [
        "for i in range(1000):\n",
        "  # forward pass\n",
        "  logits=xenc@W  # predict log counts\n",
        "  counts=logits.exp() # counts\n",
        "  probs=counts/counts.sum(1,keepdim=True)\n",
        "\n",
        "  # Computing counts = logits.exp() and then normalizing can overflow if any logit is large\n",
        "  #so we can use pytorch's inbuilt cross-entropy function\n",
        "  loss=F.cross_entropy(logits,ys) + 0.01 * (W**2).mean()\n",
        "\n",
        "  #backward pass\n",
        "  W.grad=None #set to zero\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "\n",
        "  W.data+=-2*W.grad\n",
        "  if i%10 == 0:\n",
        "    print(f\"Loss at the {i}th iteration is: {loss}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsh_251Q2T1G"
      },
      "source": [
        "# Instead of using one-hot encoding , we directly index into the row!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "kxX0qAd12QWX"
      },
      "outputs": [],
      "source": [
        "# randomly initialize 27 neurons weight ,each neuron receives 27  inputs\n",
        "g=torch.Generator().manual_seed(2147483647)\n",
        "W=torch.randn((27,27),generator=g,requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuPkILsl2FV8",
        "outputId": "c607f18d-6a88-4fc0-b4d1-5c3ecdb683bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss at the 0th iteration is: 3.7201380729675293\n",
            "\n",
            "Loss at the 10th iteration is: 3.561170816421509\n",
            "\n",
            "Loss at the 20th iteration is: 3.4315710067749023\n",
            "\n",
            "Loss at the 30th iteration is: 3.32521653175354\n",
            "\n",
            "Loss at the 40th iteration is: 3.2377700805664062\n",
            "\n",
            "Loss at the 50th iteration is: 3.165433168411255\n",
            "\n",
            "Loss at the 60th iteration is: 3.1048848628997803\n",
            "\n",
            "Loss at the 70th iteration is: 3.0534780025482178\n",
            "\n",
            "Loss at the 80th iteration is: 3.009258270263672\n",
            "\n",
            "Loss at the 90th iteration is: 2.9708192348480225\n",
            "\n",
            "Loss at the 100th iteration is: 2.937129020690918\n",
            "\n",
            "Loss at the 110th iteration is: 2.907400131225586\n",
            "\n",
            "Loss at the 120th iteration is: 2.8810102939605713\n",
            "\n",
            "Loss at the 130th iteration is: 2.8574540615081787\n",
            "\n",
            "Loss at the 140th iteration is: 2.836313247680664\n",
            "\n",
            "Loss at the 150th iteration is: 2.8172404766082764\n",
            "\n",
            "Loss at the 160th iteration is: 2.799947738647461\n",
            "\n",
            "Loss at the 170th iteration is: 2.7841923236846924\n",
            "\n",
            "Loss at the 180th iteration is: 2.769773006439209\n",
            "\n",
            "Loss at the 190th iteration is: 2.7565197944641113\n",
            "\n",
            "Loss at the 200th iteration is: 2.7442901134490967\n",
            "\n",
            "Loss at the 210th iteration is: 2.7329657077789307\n",
            "\n",
            "Loss at the 220th iteration is: 2.7224442958831787\n",
            "\n",
            "Loss at the 230th iteration is: 2.7126405239105225\n",
            "\n",
            "Loss at the 240th iteration is: 2.703481435775757\n",
            "\n",
            "Loss at the 250th iteration is: 2.694904327392578\n",
            "\n",
            "Loss at the 260th iteration is: 2.6868550777435303\n",
            "\n",
            "Loss at the 270th iteration is: 2.6792867183685303\n",
            "\n",
            "Loss at the 280th iteration is: 2.67215895652771\n",
            "\n",
            "Loss at the 290th iteration is: 2.665435791015625\n",
            "\n",
            "Loss at the 300th iteration is: 2.6590850353240967\n",
            "\n",
            "Loss at the 310th iteration is: 2.653079032897949\n",
            "\n",
            "Loss at the 320th iteration is: 2.6473922729492188\n",
            "\n",
            "Loss at the 330th iteration is: 2.6420021057128906\n",
            "\n",
            "Loss at the 340th iteration is: 2.636887788772583\n",
            "\n",
            "Loss at the 350th iteration is: 2.632030487060547\n",
            "\n",
            "Loss at the 360th iteration is: 2.627413511276245\n",
            "\n",
            "Loss at the 370th iteration is: 2.623020648956299\n",
            "\n",
            "Loss at the 380th iteration is: 2.6188371181488037\n",
            "\n",
            "Loss at the 390th iteration is: 2.6148507595062256\n",
            "\n",
            "Loss at the 400th iteration is: 2.6110482215881348\n",
            "\n",
            "Loss at the 410th iteration is: 2.6074182987213135\n",
            "\n",
            "Loss at the 420th iteration is: 2.6039507389068604\n",
            "\n",
            "Loss at the 430th iteration is: 2.600635290145874\n",
            "\n",
            "Loss at the 440th iteration is: 2.5974631309509277\n",
            "\n",
            "Loss at the 450th iteration is: 2.5944254398345947\n",
            "\n",
            "Loss at the 460th iteration is: 2.591515064239502\n",
            "\n",
            "Loss at the 470th iteration is: 2.588724374771118\n",
            "\n",
            "Loss at the 480th iteration is: 2.586045980453491\n",
            "\n",
            "Loss at the 490th iteration is: 2.5834739208221436\n",
            "\n",
            "Loss at the 500th iteration is: 2.5810019969940186\n",
            "\n",
            "Loss at the 510th iteration is: 2.578625440597534\n",
            "\n",
            "Loss at the 520th iteration is: 2.576338291168213\n",
            "\n",
            "Loss at the 530th iteration is: 2.574136257171631\n",
            "\n",
            "Loss at the 540th iteration is: 2.5720150470733643\n",
            "\n",
            "Loss at the 550th iteration is: 2.569969415664673\n",
            "\n",
            "Loss at the 560th iteration is: 2.5679962635040283\n",
            "\n",
            "Loss at the 570th iteration is: 2.566091775894165\n",
            "\n",
            "Loss at the 580th iteration is: 2.5642523765563965\n",
            "\n",
            "Loss at the 590th iteration is: 2.5624752044677734\n",
            "\n",
            "Loss at the 600th iteration is: 2.5607569217681885\n",
            "\n",
            "Loss at the 610th iteration is: 2.5590949058532715\n",
            "\n",
            "Loss at the 620th iteration is: 2.5574862957000732\n",
            "\n",
            "Loss at the 630th iteration is: 2.5559287071228027\n",
            "\n",
            "Loss at the 640th iteration is: 2.554420232772827\n",
            "\n",
            "Loss at the 650th iteration is: 2.5529582500457764\n",
            "\n",
            "Loss at the 660th iteration is: 2.5515410900115967\n",
            "\n",
            "Loss at the 670th iteration is: 2.550166130065918\n",
            "\n",
            "Loss at the 680th iteration is: 2.5488319396972656\n",
            "\n",
            "Loss at the 690th iteration is: 2.5475375652313232\n",
            "\n",
            "Loss at the 700th iteration is: 2.5462803840637207\n",
            "\n",
            "Loss at the 710th iteration is: 2.5450592041015625\n",
            "\n",
            "Loss at the 720th iteration is: 2.543872833251953\n",
            "\n",
            "Loss at the 730th iteration is: 2.542719602584839\n",
            "\n",
            "Loss at the 740th iteration is: 2.541598320007324\n",
            "\n",
            "Loss at the 750th iteration is: 2.540508270263672\n",
            "\n",
            "Loss at the 760th iteration is: 2.53944730758667\n",
            "\n",
            "Loss at the 770th iteration is: 2.5384151935577393\n",
            "\n",
            "Loss at the 780th iteration is: 2.5374104976654053\n",
            "\n",
            "Loss at the 790th iteration is: 2.5364325046539307\n",
            "\n",
            "Loss at the 800th iteration is: 2.53548002243042\n",
            "\n",
            "Loss at the 810th iteration is: 2.5345520973205566\n",
            "\n",
            "Loss at the 820th iteration is: 2.5336482524871826\n",
            "\n",
            "Loss at the 830th iteration is: 2.5327675342559814\n",
            "\n",
            "Loss at the 840th iteration is: 2.5319085121154785\n",
            "\n",
            "Loss at the 850th iteration is: 2.531071186065674\n",
            "\n",
            "Loss at the 860th iteration is: 2.5302553176879883\n",
            "\n",
            "Loss at the 870th iteration is: 2.52945876121521\n",
            "\n",
            "Loss at the 880th iteration is: 2.528681993484497\n",
            "\n",
            "Loss at the 890th iteration is: 2.527923822402954\n",
            "\n",
            "Loss at the 900th iteration is: 2.527184009552002\n",
            "\n",
            "Loss at the 910th iteration is: 2.5264620780944824\n",
            "\n",
            "Loss at the 920th iteration is: 2.525757074356079\n",
            "\n",
            "Loss at the 930th iteration is: 2.525068759918213\n",
            "\n",
            "Loss at the 940th iteration is: 2.5243959426879883\n",
            "\n",
            "Loss at the 950th iteration is: 2.5237393379211426\n",
            "\n",
            "Loss at the 960th iteration is: 2.523097276687622\n",
            "\n",
            "Loss at the 970th iteration is: 2.522470235824585\n",
            "\n",
            "Loss at the 980th iteration is: 2.5218570232391357\n",
            "\n",
            "Loss at the 990th iteration is: 2.5212576389312744\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(1000):\n",
        "\n",
        "\"\"\" When you do\n",
        "\n",
        "logits = torch.tensor(W[xs], dtype=torch.float32)\n",
        "\n",
        "it forces PyTorch to copy the data in W[xs] into a brandnew tensor that has no connection to the original W in the autograd graph. In other words:\n",
        "\n",
        "    W[xs] by itself is a view of W (or at least a tensor that still tracks gradients back to W).\n",
        "\n",
        "    But wrapping it in torch.tensor() takes its raw data (a NumPystyle array underneath) and creates a fresh tensor with requires_grad=False. That newly created tensor has no history, so when you do loss.backward(), nothing flows back into W.\n",
        "\n",
        "As a result, even though you later call loss.backward(), W.grad will remain None (or zero), because your logits was built from a detached copy. Thus the model never learns.\n",
        "\"\"\"\n",
        "  logits=W[xs]  # predict log counts\n",
        "\n",
        "  loss=F.cross_entropy(logits,ys) + 0.01 * (W**2).mean()\n",
        "\n",
        "  #backward pass\n",
        "  W.grad=None #set to zero\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "\n",
        "  W.data+=-2*W.grad\n",
        "  if i%10 == 0:\n",
        "    print(f\"Loss at the {i}th iteration is: {loss}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
