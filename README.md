# Machine Learning Projects Collection

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

Welcome to my Machine Learning Projects Collection! This repository serves as a comprehensive resource for understanding and implementing various machine learning algorithms and architectures, from fundamental concepts to cutting-edge models.

## ğŸš€ Projects

### [GPT-2](./GPT-2/)
A complete implementation of OpenAI's GPT-2 model with pre-training and fine-tuning capabilities.
- **Features**: Multi-head attention, transformer architecture, BPE tokenization
- **Use Cases**: Text generation, language modeling, transfer learning
- **Tech Stack**: PyTorch, Transformers, Hugging Face Datasets

### [NanoGPT](./NanoGPT/)
A minimal but complete implementation of the GPT architecture, designed for educational purposes.
- **Features**: Pure PyTorch, efficient training, text generation
- **Use Cases**: Learning transformer architectures, small-scale language models
- **Tech Stack**: PyTorch, tqdm, numpy

### [Neural Network From Scratch](./Neural_Network_From_Scratch/)
Building neural networks from first principles using only NumPy.
- **Features**: Backpropagation, various layers, activation functions
- **Use Cases**: Educational, understanding deep learning fundamentals
- **Tech Stack**: NumPy, Matplotlib

### [makemore](./makemore/)
Character-level language models with multiple architectures.
- **Features**: Bigram, MLP, RNN, LSTM, GRU, and Transformer models
- **Use Cases**: Text generation, language modeling experiments
- **Tech Stack**: PyTorch, tqdm, matplotlib

### [Monte Carlo Tree Search](./Monte_Carlo_Tree_Search/)
Implementation of the Monte Carlo Tree Search algorithm for game AI.
- **Features**: Game-agnostic design, visualization, parallel simulations
- **Use Cases**: Game AI, decision making, planning
- **Tech Stack**: Python, NumPy, Matplotlib

## ğŸ› ï¸ Getting Started

### Prerequisites

- Python 3.8+
- pip (Python package manager)
- Git (for cloning the repository)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/ML_Stuff.git
   cd ML_Stuff
   ```

2. **Set up a virtual environment (recommended)**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: .\venv\Scripts\activate
   ```

3. **Install project-specific dependencies**
   Each project has its own requirements. For example, to install requirements for GPT-2:
   ```bash
   cd GPT-2
   pip install -r requirements.txt
   ```

## ğŸ¯ Usage

Each project contains detailed documentation and examples in its respective directory. Here's a quick start for running any project:

1. Navigate to the project directory
2. Follow the instructions in the project's README.md
3. Run the main script or Jupyter notebook

Example for running the NanoGPT training:
```bash
cd NanoGPT
python train.py
```

## ğŸ“Š Project Structure

```
ML_Stuff/
â”œâ”€â”€ GPT-2/                  # GPT-2 implementation
â”‚   â”œâ”€â”€ pre-training/       # Pre-training scripts
â”‚   â”œâ”€â”€ notebooks/          # Jupyter notebooks for exploration
â”‚   â””â”€â”€ README.md           # Project documentation
â”‚
â”œâ”€â”€ NanoGPT/                # Minimal GPT implementation
â”‚   â”œâ”€â”€ train.py            # Training script
â”‚   â”œâ”€â”€ model.py            # Model architecture
â”‚   â””â”€â”€ README.md           # Documentation
â”‚
â”œâ”€â”€ Neural_Network_From_Scratch/  # NumPy neural networks
â”‚   â”œâ”€â”€ neural_network.py   # Core implementation
â”‚   â””â”€â”€ README.md           # Documentation
â”‚
â”œâ”€â”€ makemore/               # Character-level language models
â”‚   â”œâ”€â”€ makemore.py         # Main implementation
â”‚   â””â”€â”€ README.md           # Documentation
â”‚
â”œâ”€â”€ Monte_Carlo_Tree_Search/ # MCTS implementation
â”‚   â”œâ”€â”€ mcts.py             # Core algorithm
â”‚   â””â”€â”€ README.md           # Documentation
â”‚
â””â”€â”€ README.md               # This file
```

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. **Report bugs**: Open an issue with detailed reproduction steps
2. **Suggest features**: Share your ideas for improvements
3. **Submit code**: Fork the repo and open a pull request

Please read our [Contributing Guidelines](CONTRIBUTING.md) for more details.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“š Resources

- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [Deep Learning Book](https://www.deeplearningbook.org/)
- [CS231n: Convolutional Neural Networks](http://cs231n.stanford.edu/)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

## ğŸ™ Acknowledgments

- The open-source community for their invaluable contributions
- All the researchers and developers who have shared their knowledge
- Special thanks to the creators of PyTorch, NumPy, and other essential libraries
